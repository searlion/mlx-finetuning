{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1055c3f3",
   "metadata": {},
   "source": [
    "### GRPO Fine-Tuning with MLX LM\n",
    "\n",
    "In this notebook, we'll walk through how to fine-tune an LLM with **Group Relative Policy Optimization (GRPO)** using MLX LM. GRPO is a Reinforcement Learning algorithm similar to PPO. We'll use the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset for common sense reasoning as an example. An outline:\n",
    "\n",
    "1. Download the dataset and prepare it for the GRPO loop.\n",
    "2. Setup and run GRPO training. We will implement the full RL loop, including rollout, reward calculation, and optimization with the PPO-clip objective.\n",
    "3. Evaluate the final accuracy on the test set.\n",
    "4. Fuse the resulting adapters into the base model.\n",
    "5. Discuss tips for debugging accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397627",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "664272fb",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T13:10:19.137354Z",
     "start_time": "2025-06-19T13:10:17.991381Z"
    }
   },
   "source": [
    "!pip install mlx-lm\n",
    "!pip install matplotlib"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlx-lm in ./.venv/lib/python3.12/site-packages (0.25.2)\r\n",
      "Requirement already satisfied: mlx>=0.25.0 in ./.venv/lib/python3.12/site-packages (from mlx-lm) (0.26.1)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from mlx-lm) (2.3.0)\r\n",
      "Requirement already satisfied: transformers>=4.39.3 in ./.venv/lib/python3.12/site-packages (from transformers[sentencepiece]>=4.39.3->mlx-lm) (4.52.4)\r\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.12/site-packages (from mlx-lm) (6.31.1)\r\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from mlx-lm) (6.0.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from mlx-lm) (3.1.6)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (0.33.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (25.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (2.32.4)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (2025.5.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (4.14.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (1.1.4)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in ./.venv/lib/python3.12/site-packages (from transformers[sentencepiece]>=4.39.3->mlx-lm) (0.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->mlx-lm) (3.0.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.39.3->transformers[sentencepiece]>=4.39.3->mlx-lm) (2025.6.15)\r\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.58.4)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\r\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.3.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\r\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "dd27c693",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "We'll start by downloading an already pre-processed version of the HellaSwag dataset from [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)."
   ]
  },
  {
   "cell_type": "code",
   "id": "61698208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T13:11:03.009251Z",
     "start_time": "2025-06-19T13:10:57.715925Z"
    }
   },
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from urllib import request\n",
    "\n",
    "save_dir = \"/tmp/hellaswag\"\n",
    "\n",
    "def download_and_save(save_dir):\n",
    "    base_url = \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/main/dataset/hellaswag/\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for name in [\"train.json\", \"test.json\"]:\n",
    "        out_file = save_dir / name\n",
    "        if not out_file.exists():\n",
    "            request.urlretrieve(base_url + name, out_file)\n",
    "\n",
    "def load_json(dataset):\n",
    "    download_and_save(save_dir)\n",
    "    with open(f\"{save_dir}/{dataset}.json\", \"r\") as fid:\n",
    "        return json.load(fid)\n",
    "\n",
    "train_set, test_set = load_json(\"train\"), load_json(\"test\")\n",
    "print(f\"HellaSwag stats: {len(train_set)} training examples and {len(test_set)} test examples.\")\n",
    "print(\"An example:\\n\")\n",
    "print(json.dumps(train_set[0], indent=4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HellaSwag stats: 39905 training examples and 10042 test examples.\n",
      "An example:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Please choose the correct ending to complete the given sentence: Removing ice from car: Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then\\n\\nEnding1: , the man adds wax to the windshield and cuts it. Ending2: , a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled. Ending3: , the man puts on a christmas coat, knitted with netting. Ending4: , the man continues removing the snow on his car.\\n\\nAnswer format: ending1/ending2/ending3/ending4\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"the correct answer is ending4\",\n",
      "    \"answer\": \"ending4\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "9a514d79",
   "metadata": {},
   "source": [
    "Next, let's split the training set into a training and a validation set. We'll pull out a randomly chosen 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "id": "9b607237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:17:21.830696Z",
     "start_time": "2025-06-19T12:17:21.352682Z"
    }
   },
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(43)\n",
    "perm = np.random.permutation(len(train_set))\n",
    "valid_size = int(0.1 * len(train_set))\n",
    "valid_set = [train_set[i] for i in perm[:valid_size]]\n",
    "train_set = [train_set[i] for i in perm[valid_size:]]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "b259eb69",
   "metadata": {},
   "source": [
    "### Fine-Tune\n",
    "\n",
    "For fine-tuning, we'll use Microsoft's [Phi-3 mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). At 3.8 billion parameters, Phi-3 mini is a high-quality model that is also fast to fine-tune on most Apple silicon machines. Also, it has a [permissive MIT License](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE).\n",
    "\n",
    "First, import all the packages and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "id": "c3ff309a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:18:03.224698Z",
     "start_time": "2025-06-19T12:18:03.156749Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten, tree_unflatten, tree_map\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.tuner.lora import LoRALinear\n",
    "from mlx_lm.tuner import linear_to_lora_layers\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "87628d24",
   "metadata": {},
   "source": [
    "Next, setup the LoRA parameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "f0851dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:18:06.026461Z",
     "start_time": "2025-06-19T12:18:06.012903Z"
    }
   },
   "source": [
    "# Make a directory to save the adapter config and weights\n",
    "adapter_path = Path(\"adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config = {\n",
    " \"num_layers\": 8,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": 8,\n",
    "    \"scale\": 10.0, # This can be tuned\n",
    "    \"dropout\": 0.0,\n",
    "}}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "56fefd19",
   "metadata": {},
   "source": [
    "Next, load the models. For GRPO, we need three models:\n",
    "- `model` (π_θ): The model we are training with LoRA adapters.\n",
    "- `model_old` (π_θold): A copy of `model` used for generating rollouts. Its weights are periodically synchronized with `model`.\n",
    "- `model_ref` (π_ref): The original pretrained model, used as a reference for the KL-divergence penalty."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb0b16f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:27:13.010152Z",
     "start_time": "2025-06-19T12:26:49.028568Z"
    }
   },
   "source": [
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Load the main model and tokenizer\n",
    "model, tokenizer = load(model_path)\n",
    "\n",
    "# Load the reference model\n",
    "model_ref, _ = load(model_path)\n",
    "model_ref.freeze()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 132344.54it/s]\n",
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 55809.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.24): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.25): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.26): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.27): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.28): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.29): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.30): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.31): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (qkv_proj): Linear(input_dims=3072, output_dims=9216, bias=False)\n",
       "        (o_proj): Linear(input_dims=3072, output_dims=3072, bias=False)\n",
       "        (rope): RoPE(96, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_up_proj): Linear(input_dims=3072, output_dims=16384, bias=False)\n",
       "        (down_proj): Linear(input_dims=8192, output_dims=3072, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (norm): RMSNorm(3072, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(input_dims=3072, output_dims=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "6609c92a",
   "metadata": {},
   "source": [
    "After loading the main model, freeze its base parameters and convert the specified linear layers to LoRA layers. The LoRA adapters will be the only trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "50e1ab3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:27:43.923174Z",
     "start_time": "2025-06-19T12:27:34.680175Z"
    }
   },
   "source": [
    "# Freeze the base model\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "# Create the old model for rollouts\n",
    "model_old, _ = load(model_path)\n",
    "linear_to_lora_layers(model_old, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
    "model_old.update(model.parameters()) # Sync weights\n",
    "model_old.freeze()\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 168811.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 786,432\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "d97656ab",
   "metadata": {},
   "source": [
    "## GRPO MODIFICATION ##\n",
    "### Define the GRPO loss and training loop\n",
    "\n",
    "Here we define the core components for GRPO. This includes:\n",
    "1. A helper function to calculate the log probabilities of a sequence.\n",
    "2. The GRPO loss function, which computes the PPO-clip objective and KL penalty.\n",
    "3. The main training loop that orchestrates the RL process."
   ]
  },
  {
   "cell_type": "code",
   "id": "grpo-helpers",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:27:49.316716Z",
     "start_time": "2025-06-19T12:27:49.306669Z"
    }
   },
   "source": [
    "from mlx import nn\n",
    "\n",
    "def calculate_log_probs(model, sequences, a_toks):\n",
    "    \"\"\"Calculates the log probabilities of the generated answer tokens.\"\"\"\n",
    "    # Pass the full sequence (prompt + answer) to the model\n",
    "    logits = model(sequences)\n",
    "\n",
    "    # We only care about the logits for the answer tokens\n",
    "    # The logits for the Nth token are used to predict the (N+1)th token\n",
    "    logits = logits[:, -len(a_toks)-1:-1, :]\n",
    "    log_probs_full = nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Get the log prob of the actual answer tokens that were generated\n",
    "    log_probs = mx.take_along_axis(log_probs_full, a_toks[None, :, None], axis=-1).squeeze(-1)\n",
    "    return mx.sum(log_probs, axis=-1)\n",
    "\n",
    "def grpo_loss_fn(model, model_ref, sequences, a_toks, advantages, old_log_probs, beta, epsilon):\n",
    "    \"\"\"The GRPO loss function.\"\"\"\n",
    "    # Get log probs from the trainable model (π_θ)\n",
    "    log_probs = calculate_log_probs(model, sequences, a_toks)\n",
    "\n",
    "    # Get log probs from the reference model (π_ref) for KL penalty\n",
    "    with mx.eval():\n",
    "        log_probs_ref = calculate_log_probs(model_ref, sequences, a_toks)\n",
    "\n",
    "    # PPO-clip objective\n",
    "    ratio = mx.exp(log_probs - old_log_probs)\n",
    "    clipped_ratio = mx.clip(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "    policy_reward = mx.minimum(ratio * advantages, clipped_ratio * advantages)\n",
    "\n",
    "    # KL penalty\n",
    "    kl_div = log_probs - log_probs_ref\n",
    "\n",
    "    # The objective is to maximize this, so we return the negative for minimization\n",
    "    loss = -mx.mean(policy_reward - beta * kl_div)\n",
    "    return loss, mx.mean(policy_reward), mx.mean(kl_div)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:31:39.651592Z",
     "start_time": "2025-06-19T12:31:39.642320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pad sequences to the same length\n",
    "def pad_sequences(sequences, pad_token_id):\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            padding = mx.full((max_len - len(seq),), pad_token_id, dtype=seq.dtype)\n",
    "            padded_seq = mx.concatenate([seq, padding])\n",
    "        else:\n",
    "            padded_seq = seq\n",
    "        padded.append(padded_seq)\n",
    "    return mx.stack(padded)\n"
   ],
   "id": "82634a0fd0588e0f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "grpo-loop",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:33:47.540012Z",
     "start_time": "2025-06-19T12:33:47.525559Z"
    }
   },
   "source": [
    "def grpo_train_loop(\n",
    "    model, model_old, model_ref, tokenizer, optimizer, train_set,\n",
    "    iters=200, group_size=4, batch_size=2, epsilon=0.2, beta=0.01,\n",
    "    update_every=10, max_ans_len=4\n",
    "):\n",
    "    # Create a grad function for the trainable model\n",
    "    loss_and_grad_fn = nn.value_and_grad(model, grpo_loss_fn)\n",
    "    \n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    # Start training\n",
    "    pbar = tqdm.tqdm(range(iters))\n",
    "    for it in pbar:\n",
    "        batch_prompts = []\n",
    "        batch_answers = []\n",
    "        \n",
    "        # 1. Sample a batch of prompts\n",
    "        indices = np.random.randint(0, len(train_set), batch_size)\n",
    "        for i in indices:\n",
    "            # The last word of the output is the ground truth answer (e.g., \"ending4\")\n",
    "            prompt_text, answer_text = train_set[i][\"output\"].rsplit(\" \", maxsplit=1)\n",
    "            full_prompt = [\n",
    "                {\"role\": \"user\", \"content\": train_set[i][\"instruction\"]},\n",
    "                {\"role\": \"assistant\", \"content\": prompt_text}\n",
    "            ]\n",
    "            batch_prompts.append(full_prompt)\n",
    "            batch_answers.append(answer_text)\n",
    "        \n",
    "        # 2. Rollout: Generate G responses for each prompt using the old model\n",
    "        rollout_sequences = []\n",
    "        rollout_rewards = []\n",
    "        rollout_log_probs = []\n",
    "        rollout_a_toks = []\n",
    "\n",
    "        with mx.eval():\n",
    "            for i in range(batch_size):\n",
    "                prompt_tokens = tokenizer.apply_chat_template(batch_prompts[i], continue_final_message=True)\n",
    "                group_rewards = []\n",
    "                \n",
    "                for _ in range(group_size):\n",
    "                    # Generate a response\n",
    "                    response = generate(model_old, tokenizer, prompt_tokens, max_tokens=max_ans_len, temp=0.7)\n",
    "                    answer_tokens = tokenizer.encode(response, add_special_tokens=False)\n",
    "                    \n",
    "                    # 3. Get Reward\n",
    "                    reward = 1.0 if batch_answers[i] in response else 0.0\n",
    "                    group_rewards.append(reward)\n",
    "                    \n",
    "                    # Store data for the optimization step\n",
    "                    full_sequence = mx.array(prompt_tokens + answer_tokens)\n",
    "                    rollout_sequences.append(full_sequence)\n",
    "                    rollout_a_toks.append(mx.array(answer_tokens))\n",
    "\n",
    "                all_rewards.extend(group_rewards)\n",
    "                rollout_rewards.append(mx.array(group_rewards))\n",
    "        \n",
    "        # 4. Compute Advantages\n",
    "        advantages = []\n",
    "        for rewards in rollout_rewards:\n",
    "            mean_reward = mx.mean(rewards)\n",
    "            std_reward = mx.sqrt(mx.var(rewards)) + 1e-8 # Add epsilon for stability\n",
    "            adv = (rewards - mean_reward) / std_reward\n",
    "            advantages.append(adv)\n",
    "        \n",
    "        advantages = mx.concatenate(advantages)\n",
    "        sequences = pad_sequences(rollout_sequences, tokenizer.pad_token_id)\n",
    "        a_toks = pad_sequences(rollout_a_toks, tokenizer.pad_token_id)\n",
    "\n",
    "        # Calculate log_probs with the old model for the ratio calculation\n",
    "        with mx.eval():\n",
    "            old_log_probs = calculate_log_probs(model_old, sequences, a_toks)\n",
    "\n",
    "        # 5. Optimization Step\n",
    "        (loss, policy_reward, kl_div), grads = loss_and_grad_fn(\n",
    "            model, model_ref, sequences, a_toks, advantages, old_log_probs, beta, epsilon\n",
    "        )\n",
    "        \n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_description(f\"Loss: {np.mean(losses[-10:]):.3f}, Mean Reward: {np.mean(all_rewards[-20:]):.3f}\")\n",
    "        \n",
    "        # Sync old model weights\n",
    "        if (it + 1) % update_every == 0:\n",
    "            model_old.update(model.parameters())\n",
    "            print(f\"\\nIter {it+1}: Synced old model weights.\")\n",
    "            \n",
    "    # Final save of adapter weights\n",
    "    model.save_weights(str(adapter_path / \"adapters.safetensors\"))\n",
    "    print(\"Saved final weights to adapters/adapters.safetensors.\")\n",
    "    return losses, all_rewards"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "827d1590",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together and actually train the model. We'll use `Adam` for the optimizer and run our custom GRPO loop."
   ]
  },
  {
   "cell_type": "code",
   "id": "984516d3",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T12:33:50.923847Z",
     "start_time": "2025-06-19T12:33:50.872525Z"
    }
   },
   "source": [
    "# GRPO Hyperparameters\n",
    "learning_rate = 1e-5\n",
    "iters = 200\n",
    "group_size = 4      # G in the paper, number of responses per prompt\n",
    "batch_size = 4      # Number of prompts per iteration\n",
    "epsilon = 0.2       # PPO clip parameter\n",
    "beta = 0.02         # KL penalty coefficient\n",
    "update_every = 10   # Sync model_old every N iterations\n",
    "max_ans_len = 4     # Max tokens to generate for an answer\n",
    "\n",
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "opt = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "print(\"Starting GRPO training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the custom GRPO training loop\n",
    "losses, rewards = grpo_train_loop(\n",
    "    model=model,\n",
    "    model_old=model_old,\n",
    "    model_ref=model_ref,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=opt,\n",
    "    train_set=train_set,\n",
    "    iters=iters,\n",
    "    group_size=group_size,\n",
    "    batch_size=batch_size,\n",
    "    epsilon=epsilon,\n",
    "    beta=beta,\n",
    "    update_every=update_every,\n",
    "    max_ans_len=max_ans_len\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training finished in {end_time - start_time:.2f}s\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GRPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object does not support the context manager protocol",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     18\u001B[39m start_time = time.time()\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# Run the custom GRPO training loop\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m losses, rewards = \u001B[43mgrpo_train_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_old\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_old\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_ref\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_ref\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43miters\u001B[49m\u001B[43m=\u001B[49m\u001B[43miters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgroup_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m=\u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43mupdate_every\u001B[49m\u001B[43m=\u001B[49m\u001B[43mupdate_every\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_ans_len\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_ans_len\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     37\u001B[39m end_time = time.time()\n\u001B[32m     38\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTraining finished in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mend_time\u001B[38;5;250m \u001B[39m-\u001B[38;5;250m \u001B[39mstart_time\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33ms\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 36\u001B[39m, in \u001B[36mgrpo_train_loop\u001B[39m\u001B[34m(model, model_old, model_ref, tokenizer, optimizer, train_set, iters, group_size, batch_size, epsilon, beta, update_every, max_ans_len)\u001B[39m\n\u001B[32m     33\u001B[39m rollout_log_probs = []\n\u001B[32m     34\u001B[39m rollout_a_toks = []\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmx\u001B[49m\u001B[43m.\u001B[49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompt_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply_chat_template\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_prompts\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontinue_final_message\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: 'NoneType' object does not support the context manager protocol"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "b8d043b8",
   "metadata": {},
   "source": [
    "The adapters are saved at the end of training in `adapters.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac329358",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls adapters/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e23ee",
   "metadata": {},
   "source": [
    "Next, let's plot the training loss and the moving average of the rewards to see how well the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('GRPO Loss', color=color)\n",
    "ax1.plot(losses, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Reward (Moving Avg)', color=color)\n",
    "ax2.plot(moving_average(rewards, n=50), color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title(\"GRPO Training Loss and Reward\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f216c",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "The training and validation loss are only part of the story. For HellaSwag, we ultimately care about how good the model is at answering questions. To asses this, let's generate the actual `ending1`, `ending2`, `ending3`, or `ending4` responses with the fine-tuned model and measure the accuracy.\n",
    "\n",
    "First, let's split the last word off of each output in the test set to create a prompt without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_eval = [(t[\"instruction\"], *t[\"output\"].rsplit(\" \", maxsplit=1)) for t in test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8becd26a",
   "metadata": {},
   "source": [
    "Next, we'll generate the response for each example in the test set and compare it to the ground-truth answer to measure the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, num_test):\n",
    "    num_correct = 0\n",
    "    for prompt, completion, answer in tqdm.tqdm(test_set_eval[:num_test]):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": completion}\n",
    "        ]\n",
    "        # Use greedy decoding for evaluation\n",
    "        response = generate(model, tokenizer, prompt=messages, max_tokens=4, temp=0.0)\n",
    "        num_correct += (answer in response)\n",
    "    return num_correct / num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in eval mode for evaluation\n",
    "model.eval()\n",
    "\n",
    "# Increase this number to use more test examples\n",
    "num_test = 100\n",
    "test_acc = evaluate(model, tokenizer, num_test)\n",
    "print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbba7f",
   "metadata": {},
   "source": [
    "### Fuse Adapters\n",
    "\n",
    "Sometimes its convenient to fuse the adapters into the base model to create a single adapted model. MLX LM has a fuse script just for that.\n",
    "\n",
    "To see more options for fusing the model, including how to upload to HuggingFace [check the documentation](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37854c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlx_lm.fuse --model {model_path} --adapter-path {adapter_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349707e",
   "metadata": {},
   "source": [
    "Once the adapters are fused, we can rerun the evaluation using the fused model to make sure it worked. By default the fused model will be saved to `fused_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c45e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fused, tokenizer_fused = load(\"fused_model\")\n",
    "test_acc_fused = evaluate(model_fused, tokenizer_fused, num_test)\n",
    "print(f\"Approximate fused model test accuracy {test_acc_fused:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc7f4c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "#### Results\n",
    "\n",
    "To figure out why your GRPO fine-tuning is not working well, it's critical to plot both the loss and the average reward. \n",
    "\n",
    "**Underfitting**: The average reward is not increasing significantly and remains low. The loss may be stagnant or decreasing very slowly. This means the model isn't learning the desired behavior. You have a few options to improve the results:\n",
    "\n",
    "- **Increase the learning rate**: A higher learning rate might be needed to escape local minima.\n",
    "- **Increase `group_size` (G)**: A larger group provides a more stable estimate of the advantage, which can improve the quality of the gradients.\n",
    "- **Tune the KL penalty `beta`**: If `beta` is too high, it will prevent the model from learning, acting as an overly strong regularizer. Try decreasing it.\n",
    "- **Increase adapter capacity**: Use more `lora_layers` or a higher `rank`.\n",
    "- **Check your reward function**: Ensure the reward accurately reflects the desired outcome. For simple tasks like this, it's straightforward, but for complex tasks, this is often a source of error.\n",
    "\n",
    "**Overfitting/Instability**: The reward increases initially but then crashes, or the loss fluctuates wildly. This means the policy updates are too large and are destabilizing the model.\n",
    "\n",
    "- **Decrease the learning rate**: This is the most common fix for instability.\n",
    "- **Tune the PPO clip `epsilon`**: A smaller `epsilon` (e.g., 0.1) will make the updates more conservative.\n",
    "- **Increase the KL penalty `beta`**: A larger `beta` will pull the policy back towards the original reference model, preventing it from straying too far into unstable regions.\n",
    "- **Update `model_old` less frequently**: Increasing `update_every` can sometimes add stability.\n",
    "\n",
    "#### Memory Use\n",
    "\n",
    "RL fine-tuning can be more memory-intensive than SFT due to storing rollouts and multiple models. Here are some tips to reduce memory use:\n",
    "\n",
    "- **Reduce `batch_size` or `group_size`**: These directly control how many sequences are held in memory for each iteration.\n",
    "- **Quantization (QLoRA)**: This is highly effective. You can use a quantized base model from Hugging Face or create one with `mlx_lm.convert`.\n",
    "- **Gradient Checkpointing**: This trades computation for memory. Add `grad_checkpoint=True` when calling the training loop and pass it down to the loss function if needed (though our custom loop doesn't have this argument, it could be added).\n",
    "- **Reduce `lora_layers` or `rank`**: Fewer trainable parameters means a smaller memory footprint for gradients and optimizer states.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- To learn more about MLX check-out the [GitHub repo](http://github.com/ml-explore/mlx) and [documentation](https://ml-explore.github.io/mlx/)\n",
    "- For more on MLX LM check-out the [MLX LM documentation](https://github.com/ml-explore/mlx-examples/tree/main/llms#readme).\n",
    "- Check out the other [MLX Examples](https://github.com/ml-explore/mlx-examples/tree/main). These are great as a learning resource or to use as a starting point for a new project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
