{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1055c3f3",
   "metadata": {},
   "source": [
    "### GRPO Fine-Tuning with MLX LM\n",
    "\n",
    "In this notebook, we'll walk through how to fine-tune an LLM with **Group Relative Policy Optimization (GRPO)** using MLX LM. GRPO is a Reinforcement Learning algorithm similar to PPO. We'll use the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset for common sense reasoning as an example. An outline:\n",
    "\n",
    "1. Download the dataset and prepare it for the GRPO loop.\n",
    "2. Setup and run GRPO training. We will implement the full RL loop, including rollout, reward calculation, and optimization with the PPO-clip objective.\n",
    "3. Evaluate the final accuracy on the test set.\n",
    "4. Fuse the resulting adapters into the base model.\n",
    "5. Discuss tips for debugging accuracy and efficiency.\n",
    "\n",
    "Note: This notebook currently does not have an implementation for the reward function. Instead, it has a dummy reward logic:\n",
    "\n",
    "```python\n",
    "reward = 1.0 if batch_answers[i] in response else 0.0\n",
    "```\n",
    "\n",
    "I will add a reward function to this notebook later on and inform again when done, but please feel free to file a pull request if you would like to contribute in anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397627",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664272fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install mlx-lm\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27c693",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "We'll start by downloading an already pre-processed version of the HellaSwag dataset from [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61698208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from urllib import request\n",
    "\n",
    "save_dir = \"/tmp/hellaswag\"\n",
    "\n",
    "def download_and_save(save_dir):\n",
    "    base_url = \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/main/dataset/hellaswag/\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for name in [\"train.json\", \"test.json\"]:\n",
    "        out_file = save_dir / name\n",
    "        if not out_file.exists():\n",
    "            request.urlretrieve(base_url + name, out_file)\n",
    "\n",
    "def load_json(dataset):\n",
    "    download_and_save(save_dir)\n",
    "    with open(f\"{save_dir}/{dataset}.json\", \"r\") as fid:\n",
    "        return json.load(fid)\n",
    "\n",
    "train_set, test_set = load_json(\"train\"), load_json(\"test\")\n",
    "print(f\"HellaSwag stats: {len(train_set)} training examples and {len(test_set)} test examples.\")\n",
    "print(\"An example:\\n\")\n",
    "print(json.dumps(train_set[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514d79",
   "metadata": {},
   "source": [
    "Next, let's split the training set into a training and a validation set. We'll pull out a randomly chosen 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b607237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(43)\n",
    "perm = np.random.permutation(len(train_set))\n",
    "valid_size = int(0.1 * len(train_set))\n",
    "valid_set = [train_set[i] for i in perm[:valid_size]]\n",
    "train_set = [train_set[i] for i in perm[valid_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259eb69",
   "metadata": {},
   "source": [
    "### Fine-Tune\n",
    "\n",
    "For fine-tuning, we'll use Microsoft's [Phi-3 mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). At 3.8 billion parameters, Phi-3 mini is a high-quality model that is also fast to fine-tune on most Apple silicon machines. Also, it has a [permissive MIT License](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE).\n",
    "\n",
    "First, import all the packages and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten, tree_unflatten, tree_map\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.tuner.lora import LoRALinear\n",
    "from mlx_lm.tuner import linear_to_lora_layers\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87628d24",
   "metadata": {},
   "source": [
    "Next, setup the LoRA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0851dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to save the adapter config and weights\n",
    "adapter_path = Path(\"adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config = {\n",
    " \"num_layers\": 8,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": 8,\n",
    "    \"scale\": 10.0, # This can be tuned\n",
    "    \"dropout\": 0.0,\n",
    "}}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fefd19",
   "metadata": {},
   "source": [
    "Next, load the models. For GRPO, we need three models:\n",
    "- `model` (π_θ): The model we are training with LoRA adapters.\n",
    "- `model_old` (π_θold): A copy of `model` used for generating rollouts. Its weights are periodically synchronized with `model`.\n",
    "- `model_ref` (π_ref): The original pretrained model, used as a reference for the KL-divergence penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Load the main model and tokenizer\n",
    "model, tokenizer = load(model_path)\n",
    "\n",
    "# Load the reference model\n",
    "model_ref, _ = load(model_path)\n",
    "model_ref.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609c92a",
   "metadata": {},
   "source": [
    "After loading the main model, freeze its base parameters and convert the specified linear layers to LoRA layers. The LoRA adapters will be the only trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "# Create the old model for rollouts\n",
    "model_old, _ = load(model_path)\n",
    "linear_to_lora_layers(model_old, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
    "model_old.update(model.parameters()) # Sync weights\n",
    "model_old.freeze()\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97656ab",
   "metadata": {},
   "source": [
    "## GRPO MODIFICATION ##\n",
    "### Define the GRPO loss and training loop\n",
    "\n",
    "Here we define the core components for GRPO. This includes:\n",
    "1. A helper function to calculate the log probabilities of a sequence.\n",
    "2. The GRPO loss function, which computes the PPO-clip objective and KL penalty.\n",
    "3. The main training loop that orchestrates the RL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx import nn\n",
    "\n",
    "def calculate_log_probs(model, sequences, a_toks):\n",
    "    \"\"\"Calculates the log probabilities of the generated answer tokens.\"\"\"\n",
    "    # Pass the full sequence (prompt + answer) to the model\n",
    "    logits = model(sequences)\n",
    "\n",
    "    # Convert to log probabilities\n",
    "    log_probs_full = nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    ## Find the actual positions where answer tokens should be extracted\n",
    "    # This assumes a_toks contains the actual token IDs that were generated\n",
    "    batch_size, seq_len = sequences.shape\n",
    "    _, ans_len = a_toks.shape\n",
    "\n",
    "    # Calculate the starting position for answer tokens (assuming they're at the end)\n",
    "    start_pos = seq_len - ans_len\n",
    "\n",
    "    # Extract log probabilities for the answer portion of the sequence\n",
    "    answer_log_probs = log_probs_full[:, start_pos:start_pos+ans_len, :]\n",
    "\n",
    "    # Create indices for gathering - ensure proper shape alignment\n",
    "    indices = a_toks[:, :, None]\n",
    "\n",
    "    # Extract log probabilities for the actual answer tokens\n",
    "    selected_log_probs = mx.take_along_axis(answer_log_probs, indices, axis=-1).squeeze(-1)\n",
    "\n",
    "    # Sum log probabilities across the answer sequence\n",
    "    return mx.sum(selected_log_probs, axis=-1)\n",
    "\n",
    "def grpo_loss_fn(model, model_ref, sequences, a_toks, advantages, old_log_probs, beta, epsilon):\n",
    "    \"\"\"The GRPO loss function.\"\"\"\n",
    "    # Get log probs from the trainable model (π_θ)\n",
    "    log_probs = calculate_log_probs(model, sequences, a_toks)\n",
    "\n",
    "    # Get log probs from the reference model (π_ref) for KL penalty\n",
    "    log_probs_ref = calculate_log_probs(model_ref, sequences, a_toks)\n",
    "\n",
    "    # PPO-clip objective\n",
    "    ratio = mx.exp(log_probs - old_log_probs)\n",
    "    clipped_ratio = mx.clip(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "    policy_reward = mx.minimum(ratio * advantages, clipped_ratio * advantages)\n",
    "\n",
    "    # KL penalty\n",
    "    # Step 1: Calculate log(r) where r = π_ref / π_θ\n",
    "    # log(r) = log(π_ref) - log(π_θ)\n",
    "    log_ratio_for_kl = log_probs_ref - log_probs\n",
    "\n",
    "    # Step 2: Calculate r itself by exponentiating log(r)\n",
    "    # r = exp(log(r))\n",
    "    ratio_for_kl = mx.exp(log_ratio_for_kl)\n",
    "\n",
    "    # Step 3: Apply the paper's full formula: r - log(r) - 1\n",
    "    kl_div = ratio_for_kl - log_ratio_for_kl - 1\n",
    "\n",
    "    # The objective is to maximize this, so we return the negative for minimization\n",
    "    loss = -mx.mean(policy_reward - beta * kl_div)\n",
    "    return loss, mx.mean(policy_reward), mx.mean(kl_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82634a0fd0588e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length\n",
    "def pad_sequences(sequences, pad_token_id):\n",
    "    if not sequences:\n",
    "        return mx.array([])\n",
    "\n",
    "    # Find hte maximum length\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            padding = mx.array([pad_token_id] * (max_len - len(seq)))\n",
    "            padded_seq = mx.concatenate([seq, padding])\n",
    "\n",
    "        else:\n",
    "            padded_seq = seq\n",
    "        padded_sequences.append(padded_seq)\n",
    "\n",
    "    return mx.stack(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_train_loop(\n",
    "    model, model_old, model_ref, tokenizer, optimizer, train_set,\n",
    "    iters=200, group_size=4, batch_size=2, epsilon=0.2, beta=0.01,\n",
    "    update_every=10, max_ans_len=4\n",
    "):\n",
    "    # Create a grad function for the trainable model\n",
    "    loss_and_grad_fn = nn.value_and_grad(model, grpo_loss_fn)\n",
    "    \n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    # Start training\n",
    "    pbar = tqdm.tqdm(range(iters))\n",
    "    for it in pbar:\n",
    "        batch_prompts = []\n",
    "        batch_answers = []\n",
    "        \n",
    "        # 1. Sample a batch of prompts\n",
    "        indices = np.random.randint(0, len(train_set), batch_size)\n",
    "        for i in indices:\n",
    "            # The last word of the output is the ground truth answer (e.g., \"ending4\")\n",
    "            prompt_text, answer_text = train_set[i][\"output\"].rsplit(\" \", maxsplit=1)\n",
    "            full_prompt = [\n",
    "                {\"role\": \"user\", \"content\": train_set[i][\"instruction\"]},\n",
    "                {\"role\": \"assistant\", \"content\": prompt_text}\n",
    "            ]\n",
    "            batch_prompts.append(full_prompt)\n",
    "            batch_answers.append(answer_text)\n",
    "        \n",
    "        # 2. Rollout: Generate G responses for each prompt using the old model\n",
    "        rollout_sequences = []\n",
    "        rollout_rewards = []\n",
    "        rollout_log_probs = []\n",
    "        rollout_a_toks = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            prompt_tokens = tokenizer.apply_chat_template(batch_prompts[i], continue_final_message=True)\n",
    "            group_rewards = []\n",
    "\n",
    "            for _ in range(group_size):\n",
    "                # Generate a response\n",
    "                response = generate(model_old, tokenizer, prompt_tokens, max_tokens=max_ans_len)\n",
    "                answer_tokens = tokenizer.encode(response, add_special_tokens=False)\n",
    "\n",
    "                # 3. Get Reward\n",
    "                reward = 1.0 if batch_answers[i] in response else 0.0\n",
    "                group_rewards.append(reward)\n",
    "\n",
    "                # Store data for the optimization step\n",
    "                full_sequence = mx.array(prompt_tokens + answer_tokens)\n",
    "                rollout_sequences.append(full_sequence)\n",
    "                rollout_a_toks.append(mx.array(answer_tokens))\n",
    "\n",
    "            all_rewards.extend(group_rewards)\n",
    "            rollout_rewards.append(mx.array(group_rewards))\n",
    "        \n",
    "        # 4. Compute Advantages\n",
    "        advantages = []\n",
    "        for rewards in rollout_rewards:\n",
    "            mean_reward = mx.mean(rewards)\n",
    "            std_reward = mx.sqrt(mx.var(rewards)) + 1e-8 # Add epsilon for stability\n",
    "            adv = (rewards - mean_reward) / std_reward\n",
    "            advantages.append(adv)\n",
    "        \n",
    "        advantages = mx.concatenate(advantages)\n",
    "        sequences = pad_sequences(rollout_sequences, tokenizer.pad_token_id)\n",
    "        a_toks = pad_sequences(rollout_a_toks, tokenizer.pad_token_id)\n",
    "\n",
    "        # Calculate log_probs with the old model for the ratio calculation\n",
    "        old_log_probs = calculate_log_probs(model_old, sequences, a_toks)\n",
    "\n",
    "        # 5. Optimization Step\n",
    "        (loss, policy_reward, kl_div), grads = loss_and_grad_fn(\n",
    "            model, model_ref, sequences, a_toks, advantages, old_log_probs, beta, epsilon\n",
    "        )\n",
    "        \n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_description(f\"Loss: {np.mean(losses[-10:]):.3f}, Mean Reward: {np.mean(all_rewards[-20:]):.3f}\")\n",
    "        \n",
    "        # Sync old model weights\n",
    "        if (it + 1) % update_every == 0:\n",
    "            model_old.update(model.parameters())\n",
    "            print(f\"\\nIter {it+1}: Synced old model weights.\")\n",
    "            \n",
    "    # Final save of adapter weights\n",
    "    model.save_weights(str(adapter_path / \"adapters.safetensors\"))\n",
    "    print(\"Saved final weights to adapters/adapters.safetensors.\")\n",
    "    return losses, all_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d1590",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together and actually train the model. We'll use `Adam` for the optimizer and run our custom GRPO loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984516d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRPO Hyperparameters\n",
    "learning_rate = 1e-5\n",
    "iters = 200\n",
    "group_size = 4      # G in the paper, number of responses per prompt\n",
    "batch_size = 4      # Number of prompts per iteration\n",
    "epsilon = 0.2       # PPO clip parameter\n",
    "beta = 0.02         # KL penalty coefficient\n",
    "update_every = 10   # Sync model_old every N iterations\n",
    "max_ans_len = 4     # Max tokens to generate for an answer\n",
    "\n",
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "opt = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "print(\"Starting GRPO training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the custom GRPO training loop\n",
    "losses, rewards = grpo_train_loop(\n",
    "    model=model,\n",
    "    model_old=model_old,\n",
    "    model_ref=model_ref,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=opt,\n",
    "    train_set=train_set,\n",
    "    iters=iters,\n",
    "    group_size=group_size,\n",
    "    batch_size=batch_size,\n",
    "    epsilon=epsilon,\n",
    "    beta=beta,\n",
    "    update_every=update_every,\n",
    "    max_ans_len=max_ans_len\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training finished in {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d043b8",
   "metadata": {},
   "source": [
    "The adapters are saved at the end of training in `adapters.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac329358",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls adapters/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e23ee",
   "metadata": {},
   "source": [
    "Next, let's plot the training loss and the moving average of the rewards to see how well the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('GRPO Loss', color=color)\n",
    "ax1.plot(losses, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Reward (Moving Avg)', color=color)\n",
    "ax2.plot(moving_average(rewards, n=50), color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title(\"GRPO Training Loss and Reward\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f216c",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "The training and validation loss are only part of the story. For HellaSwag, we ultimately care about how good the model is at answering questions. To asses this, let's generate the actual `ending1`, `ending2`, `ending3`, or `ending4` responses with the fine-tuned model and measure the accuracy.\n",
    "\n",
    "First, let's split the last word off of each output in the test set to create a prompt without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_eval = [(t[\"instruction\"], *t[\"output\"].rsplit(\" \", maxsplit=1)) for t in test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8becd26a",
   "metadata": {},
   "source": [
    "Next, we'll generate the response for each example in the test set and compare it to the ground-truth answer to measure the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, num_test):\n",
    "    num_correct = 0\n",
    "    for prompt, completion, answer in tqdm.tqdm(test_set_eval[:num_test]):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": completion}\n",
    "        ]\n",
    "        # Transform the prompt into the chat template\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            conversation=messages, add_generation_prompt=True\n",
    "        )\n",
    "        # Use greedy decoding for evaluation\n",
    "        response = generate(model, tokenizer, prompt=prompt, max_tokens=4, temp=0.0)\n",
    "        num_correct += (answer in response)\n",
    "    return num_correct / num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in eval mode for evaluation\n",
    "model.eval()\n",
    "\n",
    "# Increase this number to use more test examples\n",
    "num_test = 100\n",
    "test_acc = evaluate(model, tokenizer, num_test)\n",
    "print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbba7f",
   "metadata": {},
   "source": [
    "### Fuse Adapters\n",
    "\n",
    "Sometimes its convenient to fuse the adapters into the base model to create a single adapted model. MLX LM has a fuse script just for that.\n",
    "\n",
    "To see more options for fusing the model, including how to upload to HuggingFace [check the documentation](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37854c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlx_lm.fuse --model {model_path} --adapter-path {adapter_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349707e",
   "metadata": {},
   "source": [
    "Once the adapters are fused, we can rerun the evaluation using the fused model to make sure it worked. By default the fused model will be saved to `fused_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c45e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fused, tokenizer_fused = load(\"fused_model\")\n",
    "test_acc_fused = evaluate(model_fused, tokenizer_fused, num_test)\n",
    "print(f\"Approximate fused model test accuracy {test_acc_fused:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc7f4c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "#### Results\n",
    "\n",
    "To figure out why your GRPO fine-tuning is not working well, it's critical to plot both the loss and the average reward. \n",
    "\n",
    "**Underfitting**: The average reward is not increasing significantly and remains low. The loss may be stagnant or decreasing very slowly. This means the model isn't learning the desired behavior. You have a few options to improve the results:\n",
    "\n",
    "- **Increase the learning rate**: A higher learning rate might be needed to escape local minima.\n",
    "- **Increase `group_size` (G)**: A larger group provides a more stable estimate of the advantage, which can improve the quality of the gradients.\n",
    "- **Tune the KL penalty `beta`**: If `beta` is too high, it will prevent the model from learning, acting as an overly strong regularizer. Try decreasing it.\n",
    "- **Increase adapter capacity**: Use more `lora_layers` or a higher `rank`.\n",
    "- **Check your reward function**: Ensure the reward accurately reflects the desired outcome. For simple tasks like this, it's straightforward, but for complex tasks, this is often a source of error.\n",
    "\n",
    "**Overfitting/Instability**: The reward increases initially but then crashes, or the loss fluctuates wildly. This means the policy updates are too large and are destabilizing the model.\n",
    "\n",
    "- **Decrease the learning rate**: This is the most common fix for instability.\n",
    "- **Tune the PPO clip `epsilon`**: A smaller `epsilon` (e.g., 0.1) will make the updates more conservative.\n",
    "- **Increase the KL penalty `beta`**: A larger `beta` will pull the policy back towards the original reference model, preventing it from straying too far into unstable regions.\n",
    "- **Update `model_old` less frequently**: Increasing `update_every` can sometimes add stability.\n",
    "\n",
    "#### Memory Use\n",
    "\n",
    "RL fine-tuning can be more memory-intensive than SFT due to storing rollouts and multiple models. Here are some tips to reduce memory use:\n",
    "\n",
    "- **Reduce `batch_size` or `group_size`**: These directly control how many sequences are held in memory for each iteration.\n",
    "- **Quantization (QLoRA)**: This is highly effective. You can use a quantized base model from Hugging Face or create one with `mlx_lm.convert`.\n",
    "- **Gradient Checkpointing**: This trades computation for memory. Add `grad_checkpoint=True` when calling the training loop and pass it down to the loss function if needed (though our custom loop doesn't have this argument, it could be added).\n",
    "- **Reduce `lora_layers` or `rank`**: Fewer trainable parameters means a smaller memory footprint for gradients and optimizer states.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- To learn more about MLX check-out the [GitHub repo](http://github.com/ml-explore/mlx) and [documentation](https://ml-explore.github.io/mlx/)\n",
    "- For more on MLX LM check-out the [MLX LM documentation](https://github.com/ml-explore/mlx-examples/tree/main/llms#readme).\n",
    "- Check out the other [MLX Examples](https://github.com/ml-explore/mlx-examples/tree/main). These are great as a learning resource or to use as a starting point for a new project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
